
@article{bovyExtremeDeconvolutionInferring2011,
  title = {Extreme Deconvolution: {{Inferring}} Complete Distribution Functions from Noisy, Heterogeneous and Incomplete Observations},
  volume = {5},
  shorttitle = {Extreme Deconvolution},
  abstract = {We generalize the well-known mixtures of Gaussians approach to density estimation and the accompanying Expectation\textendash{}Maximization technique for finding the maximum likelihood parameters of the mixture to the case where each data point carries an individual d-dimensional uncertainty covariance and has unique missing data properties. This algorithm reconstructs the error-deconvolved or ``underlying'' distribution function common to all samples, even when the individual data points are samples from different distributions, obtained by convolving the underlying distribution with the heteroskedastic uncertainty distribution of the data point and projecting out the missing data directions. We show how this basic algorithm can be extended with conjugate priors on all of the model parameters and a ``split-and-merge'' procedure designed to avoid local maxima of the likelihood. We demonstrate the full method by applying it to the problem of inferring the three-dimensional velocity distribution of stars near the Sun from noisy two-dimensional, transverse velocity measurements from the Hipparcos satellite.},
  language = {EN},
  number = {2B},
  journal = {The Annals of Applied Statistics},
  author = {Bovy, Jo and Hogg, David W. and Roweis, Sam T.},
  month = jun,
  year = {2011},
  keywords = {Bayesian inference,density estimation,Expectation–Maximization,missing data,multivariate estimation,noise},
  pages = {1657-1677},
  file = {/Users/james/Zotero/storage/7VS5Z64C/Bovy et al. - 2011 - Extreme deconvolution Inferring complete distribu.pdf;/Users/james/Zotero/storage/AMSWLCAM/1310562737.html}
}

@inproceedings{schubertNumericallyStableParallel2018,
  address = {{New York, NY, USA}},
  series = {{{SSDBM}} '18},
  title = {Numerically {{Stable Parallel Computation}} of ({{Co}}-){{Variance}}},
  abstract = {With the advent of big data, we see an increasing interest in computing correlations in huge data sets with both many instances and many variables. Essential descriptive statistics such as the variance, standard deviation, covariance, and correlation can suffer from a numerical instability known as "catastrophic cancellation" that can lead to problems when naively computing these statistics with a popular textbook equation. While this instability has been discussed in the literature already 50 years ago, we found that even today, some high-profile tools still employ the instable version. In this paper, we study a popular incremental technique originally proposed by Welford, which we extend to weighted covariance and correlation. We also discuss strategies for further improving numerical precision, how to compute such statistics online on a data stream, with exponential aging, with missing data, and a batch parallelization for both high performance and numerical precision. We demonstrate when the numerical instability arises, and the performance of different approaches under these conditions. We showcase applications from the classic computation of variance as well as advanced applications such as stock market analysis with exponentially weighted moving models and Gaussian mixture modeling for cluster analysis that all benefit from this approach.},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Scientific}} and {{Statistical Database Management}}},
  publisher = {{ACM}},
  author = {Schubert, Erich and Gertz, Michael},
  year = {2018},
  pages = {10:1--10:12},
  file = {/Users/james/Zotero/storage/DD7X4T93/Schubert and Gertz - 2018 - Numerically Stable Parallel Computation of (Co-)Va.pdf}
}

@inproceedings{zaheerExponentialStochasticCellular2016,
  title = {Exponential {{Stochastic Cellular Automata}} for {{Massively Parallel Inference}}},
  abstract = {We propose an embarrassingly parallel, memory efficient inference algorithm for latent variable models in which the complete data likelihood is in the exponential family. The algorithm is a stochas...},
  language = {en},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Zaheer, Manzil and Wick, Michael and Tristan, Jean-Baptiste and Smola, Alex and Steele, Guy},
  month = may,
  year = {2016},
  pages = {966-975},
  file = {/Users/james/Zotero/storage/CR64YF6B/Zaheer et al. - 2016 - Exponential Stochastic Cellular Automata for Massi.pdf;/Users/james/Zotero/storage/KV3UP66V/zaheer16.html}
}

@incollection{hosseiniMatrixManifoldOptimization2015,
  title = {Matrix {{Manifold Optimization}} for {{Gaussian Mixtures}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  publisher = {{Curran Associates, Inc.}},
  author = {Hosseini, Reshad and Sra, Suvrit},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  pages = {910--918},
  file = {/Users/james/Zotero/storage/WKL3X9C6/Hosseini and Sra - 2015 - Matrix Manifold Optimization for Gaussian Mixtures.pdf;/Users/james/Zotero/storage/3WCQNGHA/5812-matrix-manifold-optimization-for-gaussian-mixtures.html}
}

@inproceedings{sculleyWebscaleKmeansClustering2010,
  address = {{Raleigh, North Carolina, USA}},
  title = {Web-Scale k-Means Clustering},
  language = {en},
  booktitle = {Proceedings of the 19th International Conference on {{World}} Wide Web - {{WWW}} '10},
  publisher = {{ACM Press}},
  author = {Sculley, D.},
  year = {2010},
  pages = {1177},
  file = {/Users/james/Zotero/storage/WZSCRMMA/Sculley - 2010 - Web-scale k-means clustering.pdf}
}

@article{andersonImprovingGaiaParallax2018,
  title = {Improving {{Gaia Parallax Precision}} with a {{Data}}-Driven {{Model}} of {{Stars}}},
  volume = {156},
  abstract = {Converting a noisy parallax measurement into a posterior belief over distance requires inference with a prior. Usually, this prior represents beliefs about the stellar density distribution of the Milky Way. However, multiband photometry exists for a large fraction of the Gaia-TGAS Catalog and is incredibly informative about stellar distances. Here, we use 2MASS colors for 1.4 million TGAS stars to build a noise-deconvolved empirical prior distribution for stars in color\textendash{}magnitude space. This model contains no knowledge of stellar astrophysics or the Milky Way but is precise because it accurately generates a large number of noisy parallax measurements under an assumption of stationarity; that is, it is capable of combining the information from many stars. We use the Extreme Deconvolution (XD) algorithm\textemdash{}which is an empirical-Bayes approximation to a full-hierarchical model of the true parallax and photometry of every star\textemdash{}to construct this prior. The prior is combined with a TGAS likelihood to infer a precise photometric-parallax estimate and uncertainty (and full posterior) for every star. Our parallax estimates are more precise than the TGAS catalog entries by a median factor of 1.2 (14\% are more precise by a factor {$>$}2) and they are more precise than the previous Bayesian distance estimates that use spatial priors. We validate our parallax inferences using members of the Milky Way star cluster M67, which is not visible as a cluster in the TGAS parallax estimates but appears as a cluster in our posterior parallax estimates. Our results, including a parallax posterior probability distribution function for each of 1.4 million TGAS stars, are available in companion electronic tables.},
  language = {en},
  number = {4},
  journal = {The Astronomical Journal},
  author = {Anderson, Lauren and Hogg, David W. and Leistedt, Boris and {Price-Whelan}, Adrian M. and Bovy, Jo},
  month = sep,
  year = {2018},
  pages = {145},
  file = {/Users/james/Zotero/storage/WXVIEDKK/Anderson et al. - 2018 - Improving Gaia Parallax Precision with a Data-driv.pdf}
}

@article{collaborationGaiaMission2016,
  title = {The {{Gaia}} Mission},
  volume = {595},
  abstract = {Gaia is a cornerstone mission in the science programme of the EuropeanSpace Agency (ESA). The spacecraft construction was approved in 2006, following a study in which the original interferometric concept was changed to a direct-imaging approach. Both the spacecraft and the payload were built by European industry. The involvement of the scientific community focusses on data processing for which the international Gaia Data Processing and Analysis Consortium (DPAC) was selected in 2007. Gaia was launched on 19 December 2013 and arrived at its operating point, the second Lagrange point of the Sun-Earth-Moon system, a few weeks later. The commissioning of the spacecraft and payload was completed on 19 July 2014. The nominal five-year mission started with four weeks of special, ecliptic-pole scanning and subsequently transferred into full-sky scanning mode. We recall the scientific goals of Gaia and give a description of the as-built spacecraft that is currently (mid-2016) being operated to achieve these goals. We pay special attention to the payload module, the performance of which is closely related to the scientific performance of the mission. We provide a summary of the commissioning activities and findings, followed by a description of the routine operational mode. We summarise scientific performance estimates on the basis of in-orbit operations. Several intermediate Gaia data releases are planned and the data can be retrieved from the Gaia Archive, which is available through the Gaia home page. \href{http://www.cosmos.esa.int/gaia}{http://www.cosmos.esa.int/gaia}},
  language = {en},
  journal = {Astronomy and Astrophysics},
  author = {{The Gaia Collaboration}},
  month = nov,
  year = {2016},
  pages = {A1},
  file = {/Users/james/Zotero/storage/UCDENC2N/Collaboration et al. - 2016 - The Gaia mission.pdf;/Users/james/Zotero/storage/UEXIHNA7/abstract.html}
}

@article{cappeOnlineExpectationMaximization2009,
  title = {On-Line Expectation\textendash{}Maximization Algorithm for Latent Data Models},
  volume = {71},
  copyright = {\textcopyright{} 2009 Royal Statistical Society},
  abstract = {Summary. We propose a generic on-line (also sometimes called adaptive or recursive) version of the expectation\textendash{}maximization (EM) algorithm applicable to latent variable models of independent observations. Compared with the algorithm of Titterington, this approach is more directly connected to the usual EM algorithm and does not rely on integration with respect to the complete-data distribution. The resulting algorithm is usually simpler and is shown to achieve convergence to the stationary points of the Kullback\textendash{}Leibler divergence between the marginal distribution of the observation and the model distribution at the optimal rate, i.e. that of the maximum likelihood estimator. In addition, the approach proposed is also suitable for conditional (or regression) models, as illustrated in the case of the mixture of linear regressions model.},
  language = {en},
  number = {3},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  author = {Capp{\'e}, Olivier and Moulines, Eric},
  year = {2009},
  keywords = {Adaptive algorithms,Expectation–maximization,Latent data models,Mixture of regressions,On-line estimation,Polyak–Ruppert averaging,Stochastic approximation},
  pages = {593-613},
  file = {/Users/james/Zotero/storage/HBTE2WEV/Cappé and Moulines - 2009 - On-line expectation–maximization algorithm for lat.pdf;/Users/james/Zotero/storage/H4W55ZUR/j.1467-9868.2009.00698.html}
}

@article{dempsterMaximumLikelihoodIncomplete1977,
  title = {Maximum {{Likelihood}} from {{Incomplete Data}} via the {{EM Algorithm}}},
  volume = {39},
  abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
  number = {1},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
  year = {1977},
  pages = {1-38},
  file = {/Users/james/Zotero/storage/4NKUJBNX/Dempster et al. - 1977 - Maximum Likelihood from Incomplete Data via the EM.pdf}
}

@article{kingmaAdamMethodStochastic2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.6980},
  primaryClass = {cs},
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  journal = {3rd International Conference for Learning Representations},
  shorttitle = {Adam},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  journal = {arXiv:1412.6980 [cs]},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2014},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/james/Zotero/storage/62KXAI54/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf;/Users/james/Zotero/storage/8NETB3AX/1412.html}
}

@article{brownGaiaDataRelease2018,
  title = {Gaia {{Data Release}} 2. {{Summary}} of the Contents and Survey Properties},
  volume = {616},
  copyright = {\textcopyright{} ESO 2018},
  abstract = {\emph{Context.\emph{ We present the second \emph{Gaia\emph{ data release, \emph{Gaia\emph{ DR2, consisting of astrometry, photometry, radial velocities, and information on astrophysical parameters and variability, for sources brighter than magnitude 21. In addition epoch astrometry and photometry are provided for a modest sample of minor planets in the solar system.\emph{Aims.\emph{ A summary of the contents of \emph{Gaia\emph{ DR2 is presented, accompanied by a discussion on the differences with respect to \emph{Gaia\emph{ DR1 and an overview of the main limitations which are still present in the survey. Recommendations are made on the responsible use of \emph{Gaia\emph{ DR2 results.\emph{Methods.\emph{ The raw data collected with the \emph{Gaia\emph{ instruments during the first 22 months of the mission have been processed by the \emph{Gaia\emph{ Data Processing and Analysis Consortium (DPAC) and turned into this second data release, which represents a major advance with respect to \emph{Gaia\emph{ DR1 in terms of completeness, performance, and richness of the data products. \emph{Results. Gaia\emph{ DR2 contains celestial positions and the apparent brightness in \emph{G\emph{ for approximately 1.7 billion sources. For 1.3 billion of those sources, parallaxes and proper motions are in addition available. The sample of sources for which variability information is provided is expanded to 0.5 million stars. This data release contains four new elements: broad-band colour information in the form of the apparent brightness in the \emph{G\emph{\textsubscript{BP\textsubscript{ (330\textendash{}680 nm) and \emph{G\emph{\textsubscript{RP\textsubscript{ (630\textendash{}1050 nm) bands is available for 1.4 billion sources; median radial velocities for some 7 million sources are presented; for between 77 and 161 million sources estimates are provided of the stellar effective temperature, extinction, reddening, and radius and luminosity; and for a pre-selected list of 14 000 minor planets in the solar system epoch astrometry and photometry are presented. Finally, \emph{Gaia\emph{ DR2 also represents a new materialisation of the celestial reference frame in the optical, the \emph{Gaia\emph{-CRF2, which is the first optical reference frame based solely on extragalactic sources. There are notable changes in the photometric system and the catalogue source list with respect to \emph{Gaia\emph{ DR1, and we stress the need to consider the two data releases as independent. \emph{Conclusions. Gaia\emph{ DR2 represents a major achievement for the \emph{Gaia\emph{ mission, delivering on the long standing promise to provide parallaxes and proper motions for over 1 billion stars, and representing a first step in the availability of complementary radial velocity and source astrophysical information for a sample of stars in the \emph{Gaia\emph{ survey which covers a very substantial fraction of the volume of our galaxy.}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},
  language = {en},
  journal = {Astronomy \& Astrophysics},
  author = {{The Gaia Collaboration}},
  month = aug,
  year = {2018},
  pages = {A1},
  file = {/Users/james/Zotero/storage/8D42UBHQ/Brown et al. - 2018 - Gaia Data Release 2 - Summary of the contents and .pdf;/Users/james/Zotero/storage/4Y6EGITX/aa33051-18.html}
}

@article{perrymanHipparcosCatalogue1997,
  title = {The {{Hipparcos Catalogue}}},
  volume = {323},
  abstract = {The principal observational characteristics of the Hipparcos Catalogue, and a summary of its main astrometric and photometric properties, are presented. Median astrometric standard errors (in position, parallax, and annual proper motion) are in the range 0.7-0.9 milliarcsec (mas) for stars brighter than 9 mag at the catalogue epoch (J1991.25). The catalogue is a ma-terialisation of the ICRS reference system, coinciding with its principal axes at the level of {$\pm$}0.6 mas, and with proper motions consistent with an inertial system at the level of {$\pm$}0.25 mas/yr. The 118 218 constituent stars provide a mean sky density of {$\sim$} 3 stars deg -2. The catalogue is available in printed and machine-readable forms.},
  number = {1},
  journal = {Astronomy and Astrophysics},
  author = {Perryman, M A C and Lindegren, L and Kovalevsky, J. and H{\o}g, E and Bastian, U. and Bernacca, P L and Cr{\'e}z{\'e}, M. and Donati, F and Grenon, M and Grewing, M and Van Leeuwen, F and Van Der Marel, H and Mignard, F and Murray, C A and Le Poole, R S and Schrijver, H and Turon, C and Arenou, Fr{\'e}d{\'e}ric and Froeschl{\'e}, M and Petersen, C S},
  year = {1997},
  pages = {49-52},
  file = {/Users/james/Zotero/storage/KY8JIXTX/Perryman et al. - 1997 - The Hipparcos Catalogue.pdf}
}
@article{bottou2018,
  title={Optimization methods for large-scale machine learning},
  author={L{\'e}on Bottou and Frank E. Curtis and Jorge Nocedal},
  journal={SIAM Rev},
  volume={60},
  number={2},
  pages={223--311},
  year={2018}
}
@article{williams1996, 
  title={Using neural networks to model conditional multivariate densities}, 
  author={Peter M. Williams}, 
  journal={Neural Computation}, 
  volume={8}, 
  number={4}, 
  pages={843--854}, 
  year={1996}
}
@article{astropy:2013,
Author = {{The Astropy Collaboration}},
Eid = {A33},
Eprint = {1307.6212},
Journal = {Astronomy \& Astrophysics},
Keywords = {methods: data analysis, methods: miscellaneous, virtual observatory tools},
Month = oct,
Pages = {A33},
Primaryclass = {astro-ph.IM},
Title = {{Astropy: A community Python package for astronomy}},
Volume = 558,
Year = 2013,
}

@article{astropy:2018,
Author = {{The Astropy Collaboration}},
Eid = {123},
Journal = {The Astronomical Journal},
Keywords = {methods: data analysis, methods: miscellaneous, methods: statistical, reference systems, Astrophysics - Instrumentation and Methods for Astrophysics},
Month = Sep,
Pages = {123},
Primaryclass = {astro-ph.IM},
Title = {{The Astropy Project: Building an Open-science Project and Status of the v2.0 Core Package}},
Volume = {156},
Year = 2018,
}
@Misc{numpy,
  author =    {Travis Oliphant},
  title =     {{NumPy}: A guide to {NumPy}},
  year =      {2006},
  howpublished = {USA: Trelgol Publishing}
}
@InProceedings{ mckinney-proc-scipy-2010,
  author    = { Wes McKinney },
  title     = { Data Structures for Statistical Computing in Python },
  booktitle = { Proceedings of the 9th Python in Science Conference },
  pages     = { 51 - 56 },
  year      = { 2010 },
  editor    = { St\'efan van der Walt and Jarrod Millman }
}
@inproceedings{paszke2017automatic,
  title={Automatic Differentiation in {PyTorch}},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  booktitle={NIPS Autodiff Workshop},
  year={2017}
}
@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}
@article{corner,
    Author = {Daniel Foreman-Mackey},
    Title = {corner.py: Scatterplot matrices in Python},
    Journal = {The Journal of Open Source Software},
    Year = 2016,
    Volume = 24
}
@Article{Hunter:2007,
  Author    = {Hunter, J. D.},
  Title     = {Matplotlib: A 2D graphics environment},
  Journal   = {Computing in Science \& Engineering},
  Volume    = {9},
  Number    = {3},
  Pages     = {90--95},
  abstract  = {Matplotlib is a 2D graphics package used for Python for
  application development, interactive scripting, and publication-quality
  image generation across user interfaces and operating systems.},
  publisher = {IEEE COMPUTER SOC},
  year      = 2007
}

