\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{color}
\usepackage{graphicx}
\usepackage[autolanguage]{numprint}

\setcitestyle{numbers}
\setcitestyle{square}
\setcitestyle{comma}
\bibliographystyle{abbrvnat}
\setlength{\bibsep}{4pt plus 2pt minus 2pt}

\newcommand{\bx}{\mathbf{x}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bm}{\mathbf{m}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\bs}{\mathbf{s}}


\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}



\title{Scalable Extreme Deconvolution}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{
  James A. Ritchie\\
  School of Informatics\\
  University of Edinburgh\\
   \texttt{james.ritchie@ed.ac.uk} \\
  \And
  Iain Murray\\
  School of Informatics\\
  University of Edinburgh\\
   \texttt{i.murray@ed.ac.uk} \\
}

\begin{document}

\maketitle

\begin{abstract}

The Extreme Deconvolution method fits a probability density to a dataset where each observation has Gaussian noise added with a known sample-specific covariance, originally intended for use with astronomical datasets.
% The existing method of fitting the model cannot scale to new larger astronomy datasets. % -- too strong. Probably feasible, and compute only gets cheaper
The existing fitting method is batch EM, which would not normally be applied to large datasets such as the Gaia catalog, which contains noisy observations of a billion stars.
We propose two minibatch variants of extreme deconvolution, based on an online variation of the EM algorithm, and direct gradient-based optimisation of the log-likelihood, both of which can run on GPUs.
We demonstrate that these methods provide faster fitting of the model, whilst being able to scale to much larger datasets.
% TODO do we really want to end with scaling to much larger datasets? Not seeing test of scaling with N. And Bovy's code could be tweaked to accumulate updates without having everything in RAM? (And we can get EC2 machines with 1TB of RAM these days...) -- revist what contributions actually are. And if want to make other claims, plan future expts accordingly.

\end{abstract}

\section{Introduction}

Extreme deconvolution is a method that fits Gaussian Mixture Models (GMMs) to noisy data where we know the covariance of the Gaussian noise added to each sample~\cite{bovyExtremeDeconvolutionInferring2011}.
The method was originally developed to perform probabilistic density estimation on the dataset of stellar velocities produced by the Hipparcos satellite~\cite{perrymanHipparcosCatalogue1997}.
The Hipparcos catalogue consists of astrometric solutions (positions and velocities on the sky) and photometry (light intensity) for 118,218 stars, with associated noise covariances provided for each entry.

The successor to the Hipparcos mission, Gaia, aims to produce an even larger catalogue, with entries for an estimated 1 billion astronomical objects~\cite{collaborationGaiaMission2016}.
Previous work using an extreme deconvolution model on the Gaia catalogue %had to
worked with a subset of the data and restricted the number of mixture components,
%as the existing method of fitting the models cannot work with very large datasets % -- I don't think the reference says this part?
but the intention is to fit models with the full dataset
\cite{andersonImprovingGaiaParallax2018}.
The existing extreme deconvolution algorithm makes a full pass over the dataset before it can update parameters, and the reference implementation requires all the data to fit in memory. To fit such large datasets in reasonable time, we would normally use stochastic or online methods, with updates based on minibatches of data to make the methods practical on GPUs \cite{bottou2018}.

In this work, we develop two minibatch methods for fitting the extreme deconvolution model
%that can scale to the full dataset size, % -- not demonstrated here?
based on 1) an online variation of the Expectation-Maximisation (EM) algorithm, and 2) a gradient optimizer.
Our implementations can run on GPUs, and provide comparable density estimates to the existing method, whilst being much faster to train.

\section{Background}

The aim of extreme deconvolution is to perform density estimation on a noisy $d$-dimensional dataset $\{\bx_i\}_{i=0}^N$, where $\bx_i$ was generated by adding zero-mean Gaussian noise $\epsilon_i$ with known per-datapoint covariance $S_i$ to a projection $R_i$ of a true value $\bv_i$,
\begin{equation}
  \bx_i = R_i\bv_i + \epsilon_i,\quad  \epsilon_i \sim \mathcal{N}(\mathbf{0}, S_i).
\end{equation}
We assume that $\bv_i$ can be modelled by a mixture of Gaussians with $K$ components,
\begin{equation}
p(\bv_i \mid \theta) = \sum_j^K \alpha_j \,\mathcal{N}(\bv \mid \bm_j, V_j), \quad \theta = \{\alpha_j, \bm_j, V_j\}_{j=1} ^ K.
\end{equation}
Because the noise model is Gaussian and the model of the underlying density is a mixture of Gaussians, the probability of $\bx_i$ is also a Gaussian mixture.
The total log-likehood of the model is
\begin{equation}
\mathcal{L}(\theta) = \sum_i^N \log \sum_j^K \alpha_j\,\mathcal{N}(\bx_i \mid R_i\bm_j, T_{ij}), \quad T_{ij} = R_iV_jR_i^T + S_i.
\end{equation}

Missing data can be handled either by making $R_i$ rank-deficient, or by setting elements of the covariance matrix $S_i$ to very large values.

\section{Methods}

\subsection{Minibatch Expectation-Maximisation}
\label{sec:minibatch-em}
The original method of fitting the extreme deconvolution model used a modification of the Expectation-Maximisation (EM) algorithm for mixture models~\cite{dempsterMaximumLikelihoodIncomplete1977}.
Here we describe a minibatch version of this algorithm based on~\citet{cappeOnlineExpectationMaximization2009}'s online EM algorithm for latent data models.
At each iteration $t$, we compute the sufficient statistics of the latent data $\bv_i$ for each component $j$ in the minibatch of size $M$, using our current estimate of the parameters,
\begin{equation}
r_{ij} = \frac{\alpha_{j} \,\mathcal{N}(\bx_i \mid R_i\bm_j, T_{ij})}{\sum_k \alpha_k \,\mathcal{N}(\bx_i \mid R_i\bm_k, T_{ik})}, \ 
\bb_{ij} = m_j + V_j R_i^T T_{ij}^{-1}(\bx_i - R_i \bm_j), \ 
B_{ij} = V_j - V_j R_i^T T_{ij}^{-1}R_iV_j.
\end{equation}
The $r_{ij}$ term is the posterior probability of datapoint $\bx_i$ coming from component $j$.
The $\bb_{ij}$ and $B_{ij}$ terms result from the fact that $\bx_i$ and $\bv_i$ are jointly Gaussian, so the distribution of $\bv_i$ conditioned on $\bx_i$ is also Gaussian with mean $\bb_{ij}$ and covariance $B_{ij}$.
The expected sufficient statistics are then summed together over the minibatch,
\begin{equation}
q_{jt} = \sum_i r_{ijt}, \quad
\bs_{jt} = \sum_i r_{ijt} \bb_{ijt}, \quad
S_{jt} = \sum_i r_{ijt} [\bb_{ijt}\bb_{ijt}^T + B_{ijt}].
\end{equation}
Stochastic estimates $\hat{\phi}_{jt} $ of the sums of sufficient statistics over the whole dataset are then updated with a sufficiently small step size $\lambda$,
\begin{equation}
\hat{\phi}_{jt} = (1 - \lambda)\hat{\phi}_{j(t-1)} + \lambda \phi_{jt},\quad
\phi_{jt} = \{q_{jt}, \bs_{jt}, S_{jt} \},\quad
\hat{\phi}_{jt} = \{\hat{q}_{jt}, \hat{\bs}_{jt}, \hat{S}_{jt} \}. \label{eq:sums}
\end{equation}
Finally, we normalise the updated sums of expected sufficient statistics to get updated estimates of the parameters,
\begin{equation}
\alpha_{j} = \frac{\hat{q}_{jt}}{M}, \quad
\bm_{j} = \frac{\hat{\bs}_{jt}}{\hat{q}_{jt}}, \quad
V_{j} = \frac{\hat{S}_{jt}}{\hat{q}_{jt}} - \bm_{j} \bm_{j}^T.
\label{eqn:mstep}
\end{equation}
This procedure is repeated with new randomly-ordered minibatches until convergence.
If we set $\lambda=1$ and replace each minibatch with the entire dataset,
% and factorise the update for $V_j$ -- % unclear to someone new?
then the update corresponds to the original batch fitting method.
Numerically however, the update for $V_j$, as written in \eqref{eqn:mstep}, is inadvisable compared to the batch update given in \cite{bovyExtremeDeconvolutionInferring2011}. There is likely to be catastrophic cancellation if the variances of the components are small relative to the means, especially if single precision floats are used, as is standard with GPU computation.
In Appendix~\ref{apx:variance-rewrite} we show how the minibatch version of this update can be rewritten in a
% symbolically equivalent but
more numerically stable form.


\subsection{Stochastic Gradient Descent}

An alternative to EM-based methods is to optimise the log-likelihood directly.
The optimization is constrained, because the mixture weights $a_j$ are positive and sum to $1$, and the covariances $V_j$ are positive definite.
Directly fitting the log-likelihood with unconstrained gradient-based optimisers requires a transformation of the parameters to remove the constraints.
The mixture weights $\alpha_j$ can be parameterised by taking the softmax of an unconstrained vector $\mathbf{z}$, and the covariances $V_j$ represented by its lower triangular Cholesky decomposition $L_j$, where the diagonal elements $qq$ of $L_j$ are constrained positive by taking the exponential of unconstrained elements $\tilde{L}_q$,
\begin{equation}
\alpha_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}}, \quad
V_j = L_jL_j^T, \quad
(L_j)_{qq} = \exp({\tilde{L}_q}).
\end{equation}
Having removed the constraints, we can optimise the likelihood using any standard minibatch gradient-based optimiser.
% TODO we didn't invent this style of approach! Should find a citation or two where this sort
% of thing is done.

For a standard Gaussian mixture model, gradient based optimization has a scaling advantage over EM\@. There is no need to form the $D\!\times\!D$ covariance matrix $V_j$, since the Gaussian density can be evaluated directly from the Cholesky factor $L_j$ in $O(D^2)$, whereas an EM update is $O(D^3)$.
Unfortunately SGD updates are also $O(D^3)$ for the extreme deconvolution model, as we need to form the covariance $T_{ij}$ for each datapoint.

\section{Experiments}
\label{sec:experiments}

We implemented both minibatch approaches in PyTorch, and compared against the reference implementation from \citet{bovyExtremeDeconvolutionInferring2011}.
To evaluate each method, we used a random sample of rows from the Gaia DR2 source table~\cite{brownGaiaDataRelease2018}.
We selected the 5 primary astrometric features, along with the BP-RP colour and mean magnitude in the G-band.
In total there were 2 million rows.
Where data were missing, we set the field to zero and the noise variance to a large value.
We set the projection $R_i$ to the identity matrix for every sample.
This preliminary study uses only a small fraction of the full dataset size, but this allows us to fit the training data into memory, a requirement for use with the original implementation of extreme deconvolution.
We used a range of mixture component sizes $K$.
In practice we would want to select a value of $K$ by cross-validation.

The existing EM method ran on CPU, whilst the minibatch EM and SGD methods ran on GPU\@.
While the absolute times depend strongly on hardware and fine implementation details, they give a sense of the sort of times possible on current workstations, and the relative times across model sizes illustrate how the methods scale.
We used a validation set comprising $10\%$ of the rows when developing our experiments.
Final model performance was evaluated on a different held-out test set also comprising 10\% of the rows at the last stage, with no parameter selection or development done based on this set.
Details required for reproducibility are provided in Appendix~\ref{apx:repro}.

Table~\ref{results-table} reports the validation and test log-likelihoods for each method.
The values are similar, but not exactly comparable, as the effect of regularisation differs for each method.
Figure~\ref{fig:training} plots the training log-likelihood against time-rescaled epoch for $K=256$, and training time as function of mixture components $K$.
Figure~\ref{fig:projection} shows a 2-D projection from an example model with $K=256$ fitted with the minibatch-EM method.

\begin{table}{}
  \caption{Average validation log-likelihoods for the Gaia data subset for different numbers of mixture components $K$, with average test log-likelihood for the best value of $K$ by validation. Average over 10 runs with standard deviation.}
  \label{results-table}
  \centering
  \begin{tabular}{lcccc}
      \toprule
      Method     & K &  Validation     & Test\\
      \midrule
      Existing EM & 64 & $-26.10 \pm 0.03$ & - \\
      \citet{bovyExtremeDeconvolutionInferring2011} & 128 & $-25.96 \pm 0.04$ & - \\
       & 256 & $-25.76 \pm 0.02$ & - \\
       & 512 & $-25.67 \pm 0.01$ & $-25.66 \pm 0.01$ \\
      \midrule
      Minibatch EM & 64 & $-26.05 \pm 0.01$ & - \\
       & 128 & $-25.91 \pm 0.01$ & - \\
       & 256 & $-25.83 \pm 0.00$ & - \\
       & 512 & $-25.80 \pm 0.00$ & $-25.79 \pm 0.00$ \\
      \midrule
      SGD & 64 & $-25.89 \pm 0.02$ & - \\
       & 128 & $-25.77 \pm 0.02$ & - \\
       & 256 & $-25.67 \pm 0.02$ & - \\
       & 512 & $-25.59 \pm 0.02$ & $-25.57 \pm 0.02$ \\
      \bottomrule
  \end{tabular}
\end{table}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/learning.pdf}
  \caption{\textbf{Left}: Average training log-likelihood as a function of training epoch for models with $K=256$. Epochs rescaled by average training time. Error bars not visible. \textbf{Right}: Training time as a function of mixture components $K$. Error bars indicate $\pm$ 2 standard deviations.}
  \label{fig:training}
\end{figure}

\begin{figure}
  \centering
  \includegraphics{figures/density.pdf}
  \caption{Density plot showing a 2-D projection of 100000 samples drawn from a model with $K=256$ and fitted with the minibatch EM method.
  The plot shows the estimated density of star positions on the sky, and has correctly recovered the structure of the Milky Way and the Magellanic Clouds.}
  \label{fig:projection}
\end{figure}

\section{Discussion}

Our results have shown that both of our proposed methods perform comparably to the existing method of fitting extreme deconvolution models, whilst converging faster.
% and being able to work with datasets that cannot fit in memory.
% TODO I commented out as just an implementation detail? Not seeing an actual test of batch (can be accumulated in chunks) vs minibatch opt.
The results also show that using GPU-based computation speeds up fitting considerably, allowing sublinear scaling of training time with mixture component size $K$.

Further improvements to our approaches are possible.
The original paper presents a method of getting out of local maxima by splitting and merging clusters, with the split-merge criteria evaluated on the whole dataset.
It should be possible to replace the criteria with stochastic estimates, which would permit them to be used with both the SGD and minibatch EM methods.
Our approaches also add more free parameters to be selected, including learning rate and batch size.
This adds scope for hyperparameter optimisation to improve the log-likelihood.
% TODO for longer/final version. Is split-merge actually worthwhile? Seems worth
% doing quick pilot study to see if actually worth investigating further.

In this preliminary study the SGD method provided the best log-likelihood values, was faster to train, and scaled better with component size $K$.
In addition, we found SGD to be more numerically stable than minibatch-EM during training.

\bibliography{references}

\appendix

\section{Stable Covariance Update}
\label{apx:variance-rewrite}

In Section~\ref{sec:minibatch-em} describing our minibatch-EM method, we noted that the M-step update for the variance of each component $V_j$ as presented in Equation~\ref{eqn:mstep} would be prone to catastrophic cancellation as a result of taking a small difference between large values using single-precision floats.
Here we present an alternative update for $V_j$ that is less prone to numerical instability, and show that it is equivalent to Equation~\ref{eqn:mstep}.
For clarity we drop the component indicator $j$ from the parameters, and add indicators $t$ and $t-1$ to distinguish between current and previous estimates of parameters.

First, we define an adjustment operation,
\begin{align}
  \text{adjust}(V, s, \bc, \bd) &= sV + \frac{1}{2}(\bc - \bd)(\bc + \bd)^T + \frac{1}{2}(\bc + \bd)(\bc - \bd)^T  \label{eq:recentre1} \\
  &= s(V + \bc\bc^T) - \bd\bd^T,\label{eq:recentre2}
\end{align}
which can be thought of as recentering a scaled variance around a new mean.
Equation~\ref{eq:recentre1} is how we actually compute the adjustment, to minimise taking small differences between large values, whilst Equation~\ref{eq:recentre2} shows the identity we are interested in.

In the M-step at iteration $t$ of our minibatch EM approach, we compute estimates of $\hat{q}_t$, $\alpha_t$ and $\bm_t$ as before using Equations~\ref{eq:sums} and~\ref{eqn:mstep}.
We also compute minibatch-specific parameters using exact sums over the minibatch:
\begin{equation}
q_b = \sum_i^M r_i, \quad
\bm_b = \frac{\sum_i^M r_i \bx_i}{q_b},\quad V_b = \frac{\sum_i^M r_i[(\bx_i - \bb_i)(\bx_i - \bb_i)^T  + B_i]}{q_b}
\end{equation}{}
We then compute our new estimate of the variance $V_t$ as a function of the previous estimates $\{\hat{q}_{t-1}, \bm_{t-1}, V_{t-1} \}$, the minibatch values $\{q_b, \bm_b, V_b\}$, and the current estimates $\{\hat{q}_t,\bm_t \}$:
\begin{align}
V_{t} &= (1 - \lambda)\,\text{adjust}(V_{t-1}, \frac{\hat{q}_{t-1}}{\hat{q}_t}, \bm_{t-1}, \bm_t) + \lambda\, \text{adjust}(V_{b}, \frac{q_b}{\hat{q}_t}, \bm_{b}, \bm_{t}) \label{eq:update-computed}\\
&= (1 - \lambda) \left[\frac{\hat{q}_{t-1}}{\hat{q}_t} \left(V_{t-1} + \bm_{t-1}\bm_{t-1}^T \right) -\bm_t \bm_t^T \right] + \lambda\,\left[\frac{q_b}{\hat{q}_t} \left(V_{b} + \bm_{b}\bm_{b}^T \right) -\bm_t \bm_t^T \right] \\
&= (1 - \lambda)\,\left[\frac{\hat{S}_{t-1}}{\hat{q}_t} -\bm_t \bm_t^T \right] + \lambda\, \left[\frac{S_t}{\hat{q}_t} -\bm_t \bm_t^T \right] \\
&= \frac{(1- \lambda)\hat{S}_{t-1} + \lambda S_t}{\hat{q}_t} - \bm_t\bm_t^T \\
&= \frac{\hat{S}_t}{\hat{q}_t} - \bm_t\bm_t^T \label{eq:update-equiv}
\end{align}
Again, Equation~\ref{eq:update-computed} is how we actually compute the update to minimise numerical errors, whilst Equation~\ref{eq:update-equiv} shows that the update is equivalent to the covariance update defined in Equation~\ref{eqn:mstep}.
Whilst we found this update worked better in practice than a direct implementation, numerical instability is still possible if the standard deviations of the components are small enough relative to the means, and further work is needed to determine if a more stable update can be performed.

\section{Experiment Details}
\label{apx:repro}

Here we provide specific details of our experiments for reproducibility.
Code used to run the experiments will be made available.

\subsection{Dataset}

From the Gaia DR2 source table we selected the columns \texttt{RA}, \texttt{DEC}, \texttt{PARALLAX}, \texttt{PMRA}, \texttt{PMDA}, \texttt{BP\_RP} and \texttt{PHOT\_G\_MEAN\_MAG} to assemble the observed dataset $\{\bx_i\}_{i=0}^N$~\cite{brownGaiaDataRelease2018}.
Random subsampling was done by selecting rows with the value of the \texttt{RANDOM\_INDEX} column less than $\numprint{2000000}$.
Noise covariance matrices $S_i$ were assembled using the corresponding error and correlation columns for each variable.
Where a column of a row was marked as missing, the corresponding element of $\bx_i$ was set to zero, the corresponding diagonal element of $S_i$ was set to $10^{12}$, and the corresponding off-diagonal elements set to zero.
For columns which do not have associated noise, the corresponding diagonal elements of $S_i$ were set to a value of $10^{-2}$, and corresponding off-diagonal elements to zero.

\subsection{Initialisation}
For each method, initialisation of the means and weights was done using the estimated counts and centroids after 10 epochs of minibatch k-means clustering~\cite{sculleyWebscaleKmeansClustering2010}.
% TODO cite scikit learn too if using their implementation?
Covariances $V_j$ were set to the identity matrix.

\subsection{Training}

All methods were trained for a total of twenty epochs.
Both minibatch methods used a batch size of $500$.
For the minibatch-EM method, the step size $\lambda$ was set to $10^{-2}$.
For the SGD method, we used the Adam optimiser with a learning of $10^{-2}$ for the first ten epochs, reducing by a factor of 10 for the last ten epochs, and all other parameters set to the suggested defaults~\cite{kingmaAdamMethodStochastic2014}.

For numerical stability, a very small amount of regularisation was applied to the covariances $V_j$.
Using the original implementation, we set the regularisation constant $w = 10^{-3}$.
For the minibatch-EM method, we added diagonal matrix $wI$ directly to each covariance matrix after updating them.
For the SGD method, we added a penalty term $\sum_j \frac{w}{\mathrm{Trace}[V_j]}$ to the loss function.
As noted in Section~\ref{sec:experiments}, the effect of $w$ is not comparable across methods.
If we were using larger values of $w$ to prevent overfitting rather than just avoiding numerical instability, $w$ would be tuned specifically for each method.

\end{document}
