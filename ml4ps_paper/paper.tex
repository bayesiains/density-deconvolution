\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}
\usepackage{color}      % microtypography

\setcitestyle{numbers}
\setcitestyle{square}
\setcitestyle{comma}
\bibliographystyle{abbrvnat}
\setlength{\bibsep}{4pt plus 2pt minus 2pt}

\newcommand{\bx}{\mathbf{x}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bm}{\mathbf{m}}
\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}



\title{Scalable Extreme Deconvolution}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{
  James A. Ritchie\\
  School of Informatics\\
  University of Edinburgh\\
   \texttt{james.ritchie@ed.ac.uk} \\
  \And
  Iain Murray\\
  School of Informatics\\
  University of Edinburgh\\
   \texttt{i.murray@ed.ac.uk} \\
}

\begin{document}

\maketitle

\begin{abstract}

The Extreme Deconvolution method is a mixture-model based approach originally intended to perform density estimation on noisy astronomy datasets with known noise covariances for each datapoint.
Newer much larger datasets are available to which the existing method for fitting the model cannot scale to.
We propose two extensions to the extreme deconvolution method based on an online variation of the EM algorithm, and direct optimisation of the log-likelihood via SGD, both which can make use of GPU computation.
We demonstrate that these methods provide faster fitting of the model, whilst being able to scale to much larger datasets.

\end{abstract}

\section{Introduction}

Many scientific applications require an estimate of some density $p(\bx)$ to be fitted using a set of samples $\bx_i$.
In many cases the dataset is noisy, but we wish to fit our density estimate to the true underlying noiseless data.
Extreme Deconvolution is a method that allows Gaussian Mixture Models (GMMs) to be fitted to data where we know the covariance of the Gaussian noise added to each sample~\cite{bovyExtremeDeconvolutionInferring2011}.
The method was originally developed to perform density estimation on the dataset produced by the Hipparcos satellite~\cite{perrymanHipparcosCatalogue1997}.
The Hipparcos catalogue consists of astrometric solutions (positions and velocities on the sky) and photometry (light intensity) for 118,218 stars, with associated noise covariances provided for each entry.

The successor to the Hipparcos mission, Gaia, aims to produce an even larger catalogue, with entries for an estimated 1 billion astronomical objects~\cite{collaborationGaiaMission2016}.
The existing method of fitting extreme deconvolution models cannot cope with such a dataset, as it requires all of the data to fit in memory, and must make a full pass over the dataset before it can update parameters.
In this work, we propose two new methods for fitting the extreme deconvolution model that can scale to such datasets, based on an online variation of the Expectation-Maximisation (EM) algorithm, and stochastic gradient descent (SGD) on the log-likelihood.
We develop implementations that can leverage GPU-based computation, and show that they provide comparable density estimates to the existing method, whilst being much faster to train.

\section{Background}

The aim of extreme deconvolution is to perform density estimation on a noisy $d$-dimensional dataset $\{\bx_i\}_{i=0}^N$, where $\bx_i$ was generated by adding zero-mean Gaussian noise $\epsilon_i$ with known per-datapoint covariance $S_i$ to a true value $\bv_i$,
\begin{equation}
  \mathbf{x_i} = \bv_i + \epsilon_i,\quad  \epsilon_i \sim \mathcal{N}(\mathbf{0}, S_i).
\end{equation}
We assume that $\bv_i$ can be modelled by a mixture of Gaussians with $K$ components,
\begin{equation}
p(\bv_i \mid \theta) = \sum_j^K \alpha_j \mathcal{N}(\bv \mid \bm_j, V_j), \quad \theta = \{\alpha_j, \bm_j, V_j\}_{j=1} ^ K
\end{equation}
As both the noise model is Gaussian and the density estimation model is a mixture of Gaussians, the likelihood of $\bx_i$ is also a Gaussian mixture,
and the total log-likehood of the dataset is
\begin{equation}
\mathcal{L}(\theta) = \sum_i^N \log \sum_j^K \alpha_j\mathcal{N}(\bv_i \mid \mathbf{m}_j, T_{ij}), \quad T_{ij} = V_j + S_i.
\end{equation}
The original model also includes a method for handling the case where $\bx_i$ has been transformed from $\bv_i$ via a per-datapoint projection matrix $R_i$, which we omit here for simplicity.
Missing data can be handled either by making $R_i$ rank-deficient, or by setting elements of the covariance matrix $S_i$ to very large values.

\section{Methods}

\subsection{Online Expectation-Maximisation}
The original method for fitting extreme deconvolution models used a modification of the Expectation-Maximisation (EM) algorithm for mixture models~\cite{dempsterMaximumLikelihoodIncomplete1977}.
In the E-step, the expected sufficient statistics for each datapoint that need to be computed are $s_{ij} = \{q_{ij}$, $q_{ij}\bv_i$, and $q_{ij}\bv_i\bv_i^T\}$.
The M-step comprises normalisation of the sums of expected sufficient statistics to get the mixture component parameters.

\citet{cappeOnlineExpectationMaximization2009} proposed an online variation of EM for latent data models.
Their method involves computing a stochastic approximation to the sums of expected sufficient statistics.
At each iteration $t$, the sums of expected sufficient statistics $s_t = \sum_i s_{ij}$ are computed over a minibatch using the current estimate of the parameters, and the stochastic estimate $\hat{s}_t$ updated as
\begin{equation}
 \hat{s}_{t} = (1 - \lambda)\hat{s}_{t-1} + \lambda s_t
\end{equation}
where $\lambda$ is a sufficiently small step-size.
$\lambda$ should theoretically decrease at each iteration according to the Robbins-Monro conditions, but in practice we found that using a constant step-size was sufficient.
After each update, the parameters of the model are updated by normalising $\hat{s}_t$ appropriately.

\todo{More complete explanation if space.}

A direct implementation suggests that we should update $V_j$ as
\begin{equation}
V_j  = \mathrm{E}(q_{ij} \bv_ij \bv_ij^T) - \bm_j \bm_j^T
\end{equation}
This is usually acceptable when using double-precision floats, but using GPU based computation restricts us to single precision.
Performing the update of $V_j$ like this with single-precision floats will result in catastrophic cancellation if the means are sufficiently large compared to the variance.
Instead, we compute smoothed versions of the sum of squared deviation products which avoids this issue.
\begin{equation}
\end{equation}

\todo{Better justification of why this works, or alternate method.}

\subsection{Stochastic Gradient Descent}

An alternative to EM-based methods is to optimise the log-likelihood directly.
The log-likelihood has some constraints on its parameters, namely that the weight $a_j$ sum to $1$ and that the covariances $V_j$ are positive definite.
Directly fitting it with standard gradient-based optimisers requires a transformation of the parameters to remove the constraints.
The mixture weights $\alpha_j$ can be parameterised by taking the softmax of an unconstrained vector $\mathbf{z}$,
\begin{equation}
\alpha_j = \frac{e^{z_j}}{\sum_k^K e^{z_k}}.
\end{equation}
We parametrise the covariances $V_j$ via the Cholesky decomposition, 
\begin{equation}
  V_j = L_jL_j^T
\end{equation}
where $L_j$ is a lower-triangular matrix.
We also constrain the diagonal elements of $L_j$ to be positive by taking the log of them.
Note that for a standard GMM we could bypass forming the covariance altogether and evaluate the log PDF of the Normal distribution directly with the Cholesky factor.
For the extreme deconvolution model, this cannot be done, as we need to form the covariance $Tij$ for every datapoint.
Having removed the constraints, we can then take the gradient of the log-likelihood with respect to the parameters and optimise it using a standard minibatch-based optimiser.
Pre-multiplying the gradient with the inverse Fisher information matrix may be necessary to keep the covariances stable during optimisation with standard stochastic gradient descent.
In practice we found that using the Adam optimisation algorithm eliminates the need to do this~\cite{kingmaAdamMethodStochastic2014}.

\section{Experiments}

We implemented both of our methods in PyTorch.
The use of PyTorch allows us to take advantage of its automatic differentiation framework and in-built optimisers for the SGD method,
as well as allowing us to write code targeting both CPU and GPU-based computation.

To evaluate our methods, we used a subset of rows from the Gaia DR2 source table \cite{brownGaiaDataRelease2018}.
We selected the 5 primary astrometric features, along with the BP-RP colour and mean flux in the G-band.
In total there were approximately 2 million rows.
Where data were missing, we set the field to zero and the noise variance to a large value.
This is only a small fraction of the full dataset size, but this allows us to fit the training data into memory, a requirement for use with the original implementation of extreme deconvolution.
We used the same number of components $K=128$ for each model, following \cite{andersonImprovingGaiaParallax2018}.
In practice we would want to select a value of $K$ by cross-validation.

Initialisation of all models was performed by first running a minibatch version of the K-means clustering algorithm on the dataset to get estimates of the weights and means \cite{sculleyWebscaleKmeansClustering2010}.
Initial covariances were set to the identity matrix.
Early stopping was done based on a held-out validation set comprising 10\% of the rows.
Final model performance was evaluated on another held-out test set comprising another 10\% of the rows.
Table~\ref{results-table} reports the validation and test log-likelihoods for each method.

\begin{table}
  \caption{Total validation and test log-likelihoods along with average training time for the Gaia data subset. Average over 5 runs with standard deviation.}
  \label{results-table}
  \centering
  \begin{tabular}{lccc}
    \toprule
    Method     & Validation     & Test & Training Time (s) \\
    \midrule
    Existing EM & -  & -     \\
    Online EM     & - & -      \\
    SGD     & $-5.468 \times 10^6 \pm 2.46 \times 10^3$        & - & $1469 \pm 11$ \\
    \bottomrule
  \end{tabular}
\end{table}

\todo{Add plot of log-likelihood results against dataset size.}

\section{Discussion}

Our results have shown that both of our proposed methods perform comparably to the existing method of fitting extreme deconvolution models, whilst being able to work with datasets that cannot fit in memory.
The results also show that using GPU-based computation speeds up training considerably.

Further improvements to our approaches are possible.
The original paper presents a method of getting out of local maxima by splitting and merging clusters, with the split-merge criteria evaluated on the whole dataset.
It should be possible to replace the criteria with stochastic estimates, which would permit them to be used with both the SGD and online EM methods.
Our approaches also add more free parameters to be selected, including learning rate and batch size.
This adds scope for hyperparameter optimisation to improve the log-likelihood, potentially making use of Bayesian optimisation.

The question remains of which of our two methods should be used for any particular problem?
As the results show, the SGD method is faster than the online EM method, at a cost of having a slightly worse log-likelihood.
The online EM method performed better, but during training would occasionally terminate early as a result of an update forming an invalid covariance matrix, necessitating a restart.
(This issue also exists with the implementation of the existing method).
The answer will therefore depend on exactly what dataset we wish to perform density estimation on, and how accurate an estimation we require.
In practice, we would suggest using the online EM if the size of the dataset was small enough to permit several restarts, whilst an extremely large dataset would probably be better fitted with the SGD method.

\todo{Add an example plot of online-EM going wrong if space?}


\bibliography{references}

\end{document}
