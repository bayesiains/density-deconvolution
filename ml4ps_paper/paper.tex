\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[preprint]{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\setcitestyle{numbers}
\setcitestyle{square}
\setcitestyle{comma}
\bibliographystyle{abbrvnat}
\setlength{\bibsep}{4pt plus 2pt minus 2pt}

\title{Scalable Extreme Deconvolution}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{
  James A. Ritchie\\
  School of Informatics\\
  University of Edinburgh\\
   \texttt{james.ritchie@ed.ac.uk} \\
  \And
  Iain Murray\\
  School of Informatics\\
  University of Edinburgh\\
   \texttt{i.murray@ed.ac.uk} \\
}

\begin{document}

\maketitle

\begin{abstract}

The Extreme Deconvolution method is a mixture-model based approach original intended to perform density estimation on astrometry datasets.
Newer much larger datasets are available to which the existing method for fitting the model cannot scale to.
We propose two extensions to the extreme deconvolution method based on an online variation of the EM algorithm, and direct optimisation of the log-likelihood via SGD.
We demonstrate that these methods provide faster fitting of the model, whilst being able to scale to much larger datasets.

\end{abstract}

\section{Introduction}

\section{Background}

\begin{equation}
\mathcal{L}(\theta) = \sum_i \log \sum_j \alpha_j\mathcal{N}(\mathbf{w}_i \mid \mathbf{m}_j, T_{ij})
\end{equation}

\section{Methods}


\subsection{Online Expectation-Maximisation}
The original method for fitting XD models used a modification of the Expectation-Maximisation (EM) algorithm for mixture models~\cite{dempsterMaximumLikelihoodIncomplete1977}.
For the extreme deconvolution case, the expected sufficient statistics that need to be tracked are $q_{ij}$, $q_{ij}\mathbf{v}_i$, and $q_{ij}\mathbf{v}_i\mathbf{v}_i^T$.
The M-step comprises normalisation of the sums of expected sufficient statistics to get the mixture component parameters.

\citet{cappeOnlineExpectationMaximization2009} proposed an online variation of EM for latent data models.
Their method involves computing a stochastic approximation to the sums of expected sufficient statistics.
At each iteration $t$, the sums of expected sufficient statistics $s_t$ are computed over a minibatch, and the stochastic estimate $\hat{s}_t$ updated as
\begin{equation}
 \hat{s}_{t} = (1 - \lambda)\hat{s}_{t-1} + \lambda s_t
\end{equation}
where $\lambda$ is a sufficiently small step-size.
$\lambda$ should theoretically decrease at each iteration according to the Robbins-Monro conditions, but in practice we found that using a constant step-size was sufficient.
After each update of $\hat{s}_t$, we update the parameters. 

\subsection{Stochastic Gradient Descent}

An alternative to EM-based methods is to optimise the log-likelihood directly.
The log-likelihood has some constraints on its parameters, namely that the weight $a_j$ sum to $1$ and that the covariances $V_j$ are positive definite.
Directly fitting it with standard gradient-based optimisers requires a transformation of the parameters to remove the constraints.
The mixture weights $\alpha_j$ can be parameterised by taking the softmax of an unconstrained vector $\mathbf{z}$,
\begin{equation}
\alpha_j = \frac{e^{z_j}}{\sum_k^K e^{z_k}}.
\end{equation}
We parametrise the covariances $V_j$ via the Cholesky decomposition, 
\begin{equation}
  V_j = L_jL_j^T
\end{equation}
where $L_j$ is a lower-triangular matrix.
Note that for a standard GMM we could bypass forming the covariance altogether and evaluate the log PDF of the Normal distribution directly with the Cholesky factor.
For the extreme deconvolution model, this cannot be done, as we need to form the covariance $Tij$ for every datapoint.
In practice we found that using the Adam optimisation algorithm eliminates the need to do this~\cite{kingmaAdamMethodStochastic2014}.

\section{Experiments}

We implemented both of our methods in PyTorch.
The use of PyTorch allows us to take advantage of its automatic differentiation framework and in-built optimisers for the SGD method,
as well as allowing us to write code targeting both CPU and GPU-based computation.

To evaluate our methods, we used a subset of rows from the Gaia DR2 source table \cite{brownGaiaDataRelease2018}.
We selected the 5 primary astrometric features, along with the BP-RP colour and mean flux in the G-band.
In total there were approximately 2 million rows.
Where data were missing, we set the field to zero, the variance to a large value.
This is only a small fraction of the full dataset size, but this allows us to fit the training data into memory, a requirement for use with the original implementation of extreme deconvolution.
We used the same number of components $K=128$ for each model, following 
In practice we would want to select a value of $K$ by cross-validation.

Initialisation of all models was performed by first running a minibatch version of the K-means clustering algorithm on the dataset to get estimates of the weights and means \cite{sculleyWebscaleKmeansClustering2010}.
Initial covariances were set to the identity matrix.
Early stopping was done based on a held-out validation set comprising 10\% of the rows.
Final model performance was evaluated on another held-out test set also comprising 10\% of the rows.

\begin{table}
  \caption{Validation and test log-likelihoods for the Gaia data subset.}
  \label{results-table}
  \centering
  \begin{tabular}{lcc}
    \toprule
    Method     & Validation     & Test \\
    \midrule
    Existing EM & -  & -     \\
    Online EM     & - & -      \\
    SGD     & -       & -  \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Discussion}

\bibliography{references}

\end{document}
