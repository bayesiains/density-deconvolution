%!TEX root = ../aistats_paper.tex
\section{Experiments}
\label{sec:experiments}

We implemented both minibatch approaches in PyTorch, and compared against the reference implementation from \citet{bovyExtremeDeconvolutionInferring2011}.
Initial experiments were conducted using synthetic data sampled from a known mixture of Gaussians.
We then evaluated the different methods on a real application using a dataset constructed from the Gaia catalogue.

The existing EM method ran on CPU, whilst the minibatch EM and SGD methods ran on GPU\@.
While the absolute times depend strongly on hardware and fine implementation details, they give a sense of the sort of times possible on current workstations, and the relative times across model sizes and dataset sizes illustrate how the methods scale.
Full details of our experimental setup for reproducibility are available in the supplementary material.

\subsection{Synthetic Data}

\todo{Description of synthetic setup}

\todo{Table of NLL results?}

\todo{Plot: Example 2D solution}

\todo{Plot: Obvious local maxima}

\todo{Plot: Denegenerate minibatch EM solution}

\subsection{Gaia Catalogue}

To evaluate each method with a real application, we used random samples of rows from the Gaia DR2 source table~\citep{brownGaiaDataRelease2018}.
We selected the 5 primary astrometric features, along with the BP-RP colour and mean magnitude in the G-band.
We set the projection $R_i$ to the identity matrix for every sample.
Where data were missing, we set the field to zero and the noise variance to a large value.
This study uses only a small fraction of the full dataset size, but this allows us to fit the training data into memory, a requirement for use with the original implementation of extreme deconvolution.

\subsubsection{Variable K}

In our initial experiments we evaluated the effect of varying the number of mixture components $K$.
A fixed subsample of $2,000,000$ rows was used.
We used a validation set comprising $10\%$ of the rows when developing our experiments.
Final model performance was evaluated on a different held-out test set also comprising 10\% of the rows at the last stage, with no parameter selection or development done based on this set.
Table~\ref{results-table} reports the validation and test log-likelihoods for each method.
The values are similar, but not exactly comparable, as the effect of regularisation differs for each method.
Figure~\ref{fig:training} plots the training log-likelihood against time-rescaled epoch for $K=256$, and training time as function of mixture components $K$.
Figure~\ref{fig:projection} shows a 2-D projection from an example model with $K=256$ fitted with the minibatch-EM method.

\begin{table*}{}
  \caption{Average validation log-likelihoods for the Gaia data subset for different numbers of mixture components $K$, with average test log-likelihood for the best value of $K$ by validation. Average over 10 runs with standard deviation.}
  \label{results-table}
  \centering
  \begin{tabular}{lcccc}
      \toprule
      Method     & K &  Validation     & Test\\
      \midrule
      Existing EM & 64 & $-26.10 \pm 0.03$ & - \\
      \citet{bovyExtremeDeconvolutionInferring2011} & 128 & $-25.96 \pm 0.04$ & - \\
       & 256 & $-25.76 \pm 0.02$ & - \\
       & 512 & $-25.67 \pm 0.01$ & $-25.66 \pm 0.01$ \\
      \midrule
      Minibatch EM & 64 & $-26.05 \pm 0.01$ & - \\
       & 128 & $-25.91 \pm 0.01$ & - \\
       & 256 & $-25.83 \pm 0.00$ & - \\
       & 512 & $-25.80 \pm 0.00$ & $-25.79 \pm 0.00$ \\
      \midrule
      SGD & 64 & $-25.89 \pm 0.02$ & - \\
       & 128 & $-25.77 \pm 0.02$ & - \\
       & 256 & $-25.67 \pm 0.02$ & - \\
       & 512 & $-25.59 \pm 0.02$ & $-25.57 \pm 0.02$ \\
      \bottomrule
  \end{tabular}
\end{table*}

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{figures/learning.pdf}
  \caption{\textbf{Left}: Average training log-likelihood as a function of training epoch for models with $K=256$. Epochs rescaled by average training time. Error bars not visible. \textbf{Right}: Training time as a function of mixture components $K$. Error bars indicate $\pm$ 2 standard deviations.}
  \label{fig:training}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics{figures/density.pdf}
  \caption{Density plot showing a 2-D projection of 100000 samples drawn from a model with $K=256$ and fitted with the minibatch EM method.
  The plot shows the estimated density of star positions on the sky, and has correctly recovered the structure of the Milky Way and the Magellanic Clouds.}
  \label{fig:projection}
\end{figure*}

\subsubsection{Variable N}

\todo{Description of setup.}

\todo{Table of results for different values of N}

\todo{Plot of validation LL against cost-scaled epochs}

\todo{Plot of training time against N}
