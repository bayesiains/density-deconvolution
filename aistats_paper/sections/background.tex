%!TEX root = ../aistats_paper.tex
\section{Background}

The aim of extreme deconvolution is to perform density estimation on a noisy $d$-dimensional dataset $\{\bx_i\}_{i=0}^N$, where $\bx_i$ was generated by adding zero-mean Gaussian noise $\epsilon_i$ with known per-datapoint covariance $S_i$ to a projection $R_i$ of a true value $\bv_i$,
\begin{equation}
  \bx_i = R_i\bv_i + \epsilon_i,\quad  \epsilon_i \sim \mathcal{N}(\mathbf{0}, S_i).
\end{equation}
We assume that $\bv_i$ can be modelled by a mixture of Gaussians with $K$ components,
\begin{multline}
p(\bv_i \mid \theta) = \sum_j^K \alpha_j \,\mathcal{N}(\bv \mid \bm_j, V_j), \\ \theta = \{\alpha_j, \bm_j, V_j\}_{j=1} ^ K.
\end{multline}
As the noise model is Gaussian and the model of the underlying density is a mixture of Gaussians, the probability of $\bx_i$ is also a Gaussian mixture.
The total log-likehood of the model is
\begin{align}
\mathcal{L}(\theta) &= \sum_i^N \log \sum_j^K \alpha_j\,\mathcal{N}(\bx_i \mid R_i\bm_j, T_{ij}), \\
T_{ij} &= R_iV_jR_i^T + S_i.
\end{align}
Missing data can be handled either by making $R_i$ rank-deficient, or by setting elements of the covariance matrix $S_i$ to very large values.
