\section{Introduction}

Extreme deconvolution is a method that fits Gaussian Mixture Models (GMMs) to noisy data where we know the covariance of the Gaussian noise added to each sample~\citep{bovyExtremeDeconvolutionInferring2011}.
The method was originally developed to perform probabilistic density estimation on the dataset of stellar velocities produced by the Hipparcos satellite~\citep{perrymanHipparcosCatalogue1997}.
The Hipparcos catalogue consists of astrometric solutions (positions and velocities on the sky) and photometry (light intensity) for 118,218 stars, with associated noise covariances provided for each entry.

The successor to the Hipparcos mission, Gaia, aims to produce an even larger catalogue, with entries for an estimated 1 billion astronomical objects~\citep{collaborationGaiaMission2016}.
Previous work using an extreme deconvolution model on the Gaia catalogue %had to
worked with a subset of the data and restricted the number of mixture components,
%as the existing method of fitting the models cannot work with very large datasets % -- I don't think the reference says this part?
but the intention is to fit models with the full dataset
\citep{andersonImprovingGaiaParallax2018}.
The existing extreme deconvolution algorithm makes a full pass over the dataset before it can update parameters, and the reference implementation requires all the data to fit in memory. To fit such large datasets in reasonable time, we would normally use stochastic or online methods, with updates based on minibatches of data to make the methods practical on GPUs \citep{bottou2018}.

In this work, we develop two minibatch methods for fitting the extreme deconvolution model
%that can scale to the full dataset size, % -- not demonstrated here?
based on 1) an online variation of the Expectation-Maximisation (EM) algorithm, and 2) a gradient optimizer.
Our implementations can run on GPUs, and provide comparable density estimates to the existing method, whilst being much faster to train.
