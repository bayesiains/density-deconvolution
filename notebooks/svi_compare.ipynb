{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "\n",
    "import corner\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb901c6012e3472a8d3244cdadae23ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '$P(\\\\mathbf{z})$')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means = torch.Tensor([\n",
    "    [0.0, 0.0],\n",
    "    [2, 3],\n",
    "    [2, -3]\n",
    "])\n",
    "covars = torch.Tensor([\n",
    "    [\n",
    "        [0.1, 0],\n",
    "        [0, 1.5]\n",
    "    ],\n",
    "    [\n",
    "        [1, 0],\n",
    "        [0, 0.1]\n",
    "    ],\n",
    "    [\n",
    "        [1, 0],\n",
    "        [0, 0.1]\n",
    "    ]\n",
    "])\n",
    "\n",
    "Z = torch.distributions.MultivariateNormal(loc=means, covariance_matrix=covars).sample((50000,)).reshape((-1, 2))\n",
    "idx = torch.randperm(Z.shape[0])\n",
    "Z = Z[idx]\n",
    "Z_train = Z[:Z.shape[0] // 2]\n",
    "Z_test = Z[Z.shape[0] // 2:]\n",
    "\n",
    "x_lim = (-2, 6)\n",
    "y_lim = (-6, 6)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "corner.hist2d(Z_train[:, 0].numpy(), Z_train[:, 1].numpy(), ax=ax)\n",
    "ax.set_xlim(x_lim)\n",
    "ax.set_ylim(y_lim)\n",
    "ax.set_title(r'$P(\\mathbf{z})$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5113deeee35c44ecb9e78ce22a59956c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '$P(\\\\mathbf{x})$')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = torch.Tensor([\n",
    "    [0.1, 0],\n",
    "    [0, 3]\n",
    "])\n",
    "X = Z + torch.distributions.MultivariateNormal(loc=torch.Tensor([0.0, 0.0]), covariance_matrix=S).sample((Z.shape[0],))\n",
    "X_train = X[:X.shape[0] // 2]\n",
    "X_test = X[X.shape[0] // 2:]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "corner.hist2d(X_train[:, 0].numpy(), X_train[:, 1].numpy())\n",
    "ax.set_xlim(x_lim)\n",
    "ax.set_ylim(y_lim)\n",
    "ax.set_title(r'$P(\\mathbf{x})$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deconv.gmm.data import DeconvDataset\n",
    "\n",
    "train_data = DeconvDataset(X_train, S.repeat(X_train.shape[0], 1, 1))\n",
    "test_data = DeconvDataset(X_test, S.repeat(X_test.shape[0], 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from deconv.flow.svi import SVIFlow\n",
    "svi = SVIFlow(\n",
    "    2,\n",
    "    5,\n",
    "    device=torch.device('cuda'),\n",
    "    batch_size=512,\n",
    "    epochs=25,\n",
    "    lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: -5.088949020182292\n",
      "Epoch 1, Train Loss: -4.314747986653646\n",
      "Epoch 2, Train Loss: -4.127806844889323\n",
      "Epoch 3, Train Loss: -4.1097688818359375\n",
      "Epoch 4, Train Loss: -4.100813252766927\n",
      "Epoch 5, Train Loss: -4.0980733203125\n",
      "Epoch 6, Train Loss: -4.094063190104166\n",
      "Epoch 7, Train Loss: -4.089927169596354\n",
      "Epoch 8, Train Loss: -4.0882167464192705\n",
      "Epoch 9, Train Loss: -4.0847424308268225\n",
      "Epoch 10, Train Loss: -4.083275260416666\n",
      "Epoch 11, Train Loss: -4.083978240559896\n",
      "Epoch 12, Train Loss: -4.080865526529948\n",
      "Epoch 13, Train Loss: -4.0820182763671875\n",
      "Epoch 14, Train Loss: -4.080985008138021\n",
      "Epoch 15, Train Loss: -4.078597290039062\n",
      "Epoch 16, Train Loss: -4.079479127604166\n",
      "Epoch 17, Train Loss: -4.079202639973959\n",
      "Epoch 18, Train Loss: -4.079061813151042\n",
      "Epoch 19, Train Loss: -4.079924132486979\n",
      "Epoch 20, Train Loss: -4.078985638020833\n",
      "Epoch 21, Train Loss: -4.07660890625\n",
      "Epoch 22, Train Loss: -4.078517631022136\n",
      "Epoch 23, Train Loss: -4.076684321289062\n",
      "Epoch 24, Train Loss: -4.0764507942708335\n"
     ]
    }
   ],
   "source": [
    "svi.fit(train_data, val_data=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils\n",
    "\n",
    "optimiser = torch.optim.Adam(params=svi.model._prior.parameters(), lr=1e-4)\n",
    "loader = data_utils.DataLoader(\n",
    "    Z_train,\n",
    "    batch_size=512,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "for i in range(50):\n",
    "    svi.model._prior.train()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for j, d in enumerate(loader):\n",
    "        d = d.to(svi.device)\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "        log_p = svi.model._prior.log_prob(d)\n",
    "        loss = -1 * torch.mean(log_p)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        train_loss += torch.sum(log_p).item()\n",
    "        \n",
    "    print('Epoch {}, Train Loss: {}'.format(i, train_loss / Z_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_samples = svi.sample_prior(10000)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "corner.hist2d(prior_samples[:, 0].cpu().numpy(), prior_samples[:, 1].cpu().numpy(), ax=ax)\n",
    "ax.set_xlim(x_lim)\n",
    "ax.set_ylim(y_lim)\n",
    "ax.set_title('Prior fitted directly to $\\mathbf{z}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for param in svi.model._prior.parameters():\n",
    "    param.requires_grad = False\n",
    "svi.fit(train_data, val_data=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_point = (\n",
    "    torch.Tensor([[3.0, 0.0]]).to(svi.device),\n",
    "    torch.cholesky(torch.Tensor([[\n",
    "        [0.1, 0],\n",
    "        [0, 3]\n",
    "    ]])).to(svi.device)\n",
    ")\n",
    "ctx = svi.model._inputs_encoder(test_point)\n",
    "\n",
    "posterior_samples = torch.zeros((10000, 2)).cpu()\n",
    "\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "for i in range(10000 // 500):\n",
    "    with torch.no_grad():\n",
    "        start = i * 500\n",
    "        stop = (i + 1) * 500\n",
    "        posterior_samples[start:stop, :] = svi.model._approximate_posterior.sample(500, context=ctx).cpu()[0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deconv.gmm.plotting import plot_covariance\n",
    "fig, ax = plt.subplots()\n",
    "corner.hist2d(posterior_samples[:, 0].numpy(), posterior_samples[:, 1].numpy(), ax=ax)\n",
    "ax.set_xlim(-2, 7)\n",
    "ax.set_ylim(-5, 7)\n",
    "plot_covariance(\n",
    "    np.array([3.0, 0.0]),\n",
    "    np.array([\n",
    "        [0.1, 0],\n",
    "        [0, 3]\n",
    "    ]),\n",
    "    ax=ax,\n",
    "    color='r'\n",
    ")\n",
    "ax.set_xlim(x_lim)\n",
    "ax.set_ylim(y_lim)\n",
    "ax.set_title('Posterior for test point after fitting with frozen pretrained prior')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in svi.model._prior.parameters():\n",
    "    param.requires_grad = True\n",
    "svi.fit(train_data, val_data=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_point = (\n",
    "    torch.Tensor([[2.0, 0]]).to(svi.device),\n",
    "    torch.cholesky(torch.Tensor([[\n",
    "        [0.1, 0],\n",
    "        [0, 3]\n",
    "    ]])).to(svi.device)\n",
    ")\n",
    "ctx = svi.model._inputs_encoder(test_point)\n",
    "\n",
    "posterior_samples = torch.zeros((10000, 2)).cpu()\n",
    "\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "for i in range(10000 // 500):\n",
    "    with torch.no_grad():\n",
    "        start = i * 500\n",
    "        stop = (i + 1) * 500\n",
    "        posterior_samples[start:stop, :] = svi.model._approximate_posterior.sample(500, context=ctx).cpu()[0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prior_samples = svi.model._prior.sample(10000)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "corner.hist2d(prior_samples[:, 0].cpu().numpy(), prior_samples[:, 1].cpu().numpy(), ax=ax)\n",
    "ax.set_xlim(x_lim)\n",
    "ax.set_ylim(y_lim)\n",
    "ax.set_title('Prior after joint fitting with posterior')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deconv.gmm.plotting import plot_covariance\n",
    "fig, ax = plt.subplots()\n",
    "corner.hist2d(posterior_samples[:, 0].numpy(), posterior_samples[:, 1].numpy(), ax=ax)\n",
    "ax.set_xlim(x_lim)\n",
    "ax.set_ylim(y_lim)\n",
    "plot_covariance(\n",
    "    np.array([2.0, 0]),\n",
    "    np.array([\n",
    "        [0.1, 0],\n",
    "        [0, 3]\n",
    "    ]),\n",
    "    ax=ax,\n",
    "    color='r'\n",
    ")\n",
    "ax.set_xlim(x_lim)\n",
    "ax.set_ylim(y_lim)\n",
    "ax.set_title('Posterior for test point after joint fitting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deconv.flow.svi_mdn import SVIMDNFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi_mdn = SVIMDNFlow(\n",
    "    2,\n",
    "    5,\n",
    "    device=torch.device('cuda'),\n",
    "    batch_size=512,\n",
    "    epochs=100,\n",
    "    lr=1e-4,\n",
    "    kl_warmup=0.0,\n",
    "    kl_init_factor=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils\n",
    "\n",
    "optimiser = torch.optim.Adam(params=svi_mdn.model._prior.parameters(), lr=1e-4)\n",
    "loader = data_utils.DataLoader(\n",
    "    Z_train,\n",
    "    batch_size=512,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "for i in range(50):\n",
    "    svi_mdn.model._prior.train()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for j, d in enumerate(loader):\n",
    "        d = d.to(svi_mdn.device)\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "        log_p = svi_mdn.model._prior.log_prob(d)\n",
    "        loss = -1 * torch.mean(log_p)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        train_loss += torch.sum(log_p).item()\n",
    "        \n",
    "    print('Epoch {}, Train Loss: {}'.format(i, train_loss / Z_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in svi_mdn.model._prior.parameters():\n",
    "    param.requires_grad = False\n",
    "svi_mdn.fit(train_data, val_data=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "with torch.no_grad():\n",
    "    prior_samples = svi_mdn.model._prior.sample(10000)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "corner.hist2d(prior_samples[:, 0].cpu().numpy(), prior_samples[:, 1].cpu().numpy(), ax=ax)\n",
    "ax.set_xlim(x_lim)\n",
    "ax.set_ylim(y_lim)\n",
    "ax.set_title('Prefitted Prior')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_point = (\n",
    "    torch.Tensor([[3.0, -6.0]]).to(svi_mdn.device),\n",
    "    torch.cholesky(torch.Tensor([[\n",
    "        [0.1, 0],\n",
    "        [0, 3]\n",
    "    ]])).to(svi_mdn.device)\n",
    ")\n",
    "ctx = svi_mdn.model._inputs_encoder(test_point)\n",
    "\n",
    "posterior_samples = torch.zeros((10000, 2)).cpu()\n",
    "\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "for i in range(10000 // 500):\n",
    "    with torch.no_grad():\n",
    "        start = i * 500\n",
    "        stop = (i + 1) * 500\n",
    "        posterior_samples[start:stop, :] = svi_mdn.model._approximate_posterior.sample(500, context=ctx).cpu()[0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deconv.gmm.plotting import plot_covariance\n",
    "fig, ax = plt.subplots()\n",
    "corner.hist2d(posterior_samples[:, 0].numpy(), posterior_samples[:, 1].numpy(), ax=ax)\n",
    "ax.set_xlim(x_lim)\n",
    "ax.set_ylim(y_lim)\n",
    "plot_covariance(\n",
    "    test_point[0].cpu().numpy()[0],\n",
    "    np.array([\n",
    "        [0.1, 0],\n",
    "        [0, 3]\n",
    "    ]),\n",
    "    ax=ax,\n",
    "    color='r'\n",
    ")\n",
    "ax.set_xlim(x_lim)\n",
    "ax.set_ylim(y_lim)\n",
    "ax.set_title('Posterior for test point with MDN and pretrained prior.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_point[0].cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
